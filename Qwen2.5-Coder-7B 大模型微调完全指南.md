# Qwen2.5-Coder-7B å¤§æ¨¡å‹å¾®è°ƒå®Œå…¨æŒ‡å—

> åŸºäºRunPodç¯å¢ƒä¸LLaMA-Factoryæ¡†æ¶çš„å…¨å‚æ•°å¾®è°ƒå®æˆ˜æ•™ç¨‹

## ç›®å½•

- [1. ç¯å¢ƒå‡†å¤‡](#1-ç¯å¢ƒå‡†å¤‡)
  - [1.1 RunPodç¯å¢ƒé…ç½®](#11-runpodç¯å¢ƒé…ç½®)
  - [1.2 LLaMA-Factoryå®‰è£…](#12-llama-factoryå®‰è£…)
  - [1.3 ä¾èµ–é¡¹å®‰è£…](#13-ä¾èµ–é¡¹å®‰è£…)
- [2. æ¨¡å‹ä¸æ•°æ®å‡†å¤‡](#2-æ¨¡å‹ä¸æ•°æ®å‡†å¤‡)
  - [2.1 æ¨¡å‹ä¸‹è½½](#21-æ¨¡å‹ä¸‹è½½)
  - [2.2 æ•°æ®é›†å‡†å¤‡](#22-æ•°æ®é›†å‡†å¤‡)
  - [2.3 æ•°æ®æ ¼å¼è¦æ±‚](#23-æ•°æ®æ ¼å¼è¦æ±‚)
- [3. è®­ç»ƒé…ç½®è¯¦è§£](#3-è®­ç»ƒé…ç½®è¯¦è§£)
  - [3.1 å…¨å‚æ•°å¾®è°ƒvs LoRA](#31-å…¨å‚æ•°å¾®è°ƒvs-lora)
  - [3.2 è®­ç»ƒè„šæœ¬é…ç½®](#32-è®­ç»ƒè„šæœ¬é…ç½®)
  - [3.3 DeepSpeedé…ç½®](#33-deepspeedé…ç½®)
  - [3.4 å¤šGPUè®­ç»ƒè®¾ç½®](#34-å¤šgpuè®­ç»ƒè®¾ç½®)
- [4. è®­ç»ƒè¿‡ç¨‹ç®¡ç†](#4-è®­ç»ƒè¿‡ç¨‹ç®¡ç†)
  - [4.1 ç£ç›˜ç©ºé—´ç®¡ç†](#41-ç£ç›˜ç©ºé—´ç®¡ç†)
  - [4.2 æ£€æŸ¥ç‚¹ç®¡ç†](#42-æ£€æŸ¥ç‚¹ç®¡ç†)
  - [4.3 è®­ç»ƒè¿‡ç¨‹ç›‘æ§](#43-è®­ç»ƒè¿‡ç¨‹ç›‘æ§)
  - [4.4 ä½¿ç”¨tmuxè¿›è¡Œåå°è®­ç»ƒ](#44-ä½¿ç”¨tmuxè¿›è¡Œåå°è®­ç»ƒ)
- [5. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#5-å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)
  - [5.1 æ˜¾å­˜ä¸è¶³](#51-æ˜¾å­˜ä¸è¶³)
  - [5.2 è®­ç»ƒä¸­æ–­æ¢å¤](#52-è®­ç»ƒä¸­æ–­æ¢å¤)
  - [5.3 æ•°æ®å¤„ç†é—®é¢˜](#53-æ•°æ®å¤„ç†é—®é¢˜)
  - [5.4 æ¨¡å‹ä¿å­˜ä¸åŠ è½½](#54-æ¨¡å‹ä¿å­˜ä¸åŠ è½½)
- [6. è¿›é˜¶åŠŸèƒ½](#6-è¿›é˜¶åŠŸèƒ½)
  - [6.1 Telegramè®­ç»ƒç›‘æ§](#61-telegramè®­ç»ƒç›‘æ§)
  - [6.2 è‡ªåŠ¨æ¸…ç†æ£€æŸ¥ç‚¹](#62-è‡ªåŠ¨æ¸…ç†æ£€æŸ¥ç‚¹)
  - [6.3 è®­ç»ƒåè¯„ä¼°](#63-è®­ç»ƒåè¯„ä¼°)
- [7. é™„å½•](#7-é™„å½•)
  - [7.1 å®Œæ•´è®­ç»ƒè„šæœ¬](#71-å®Œæ•´è®­ç»ƒè„šæœ¬)
  - [7.2 ç›‘æ§è„šæœ¬](#72-ç›‘æ§è„šæœ¬)
  - [7.3 èµ„æºé“¾æ¥](#73-èµ„æºé“¾æ¥)

## 1. ç¯å¢ƒå‡†å¤‡

### 1.1 RunPodç¯å¢ƒé…ç½®

RunPodæ˜¯ä¸€ä¸ªæä¾›GPUè®¡ç®—èµ„æºçš„äº‘å¹³å°ï¼Œéå¸¸é€‚åˆå¤§å‹æ¨¡å‹çš„è®­ç»ƒã€‚å¯¹äºQwen2.5-Coder-7Bå…¨å‚æ•°å¾®è°ƒï¼Œæ¨èä»¥ä¸‹é…ç½®ï¼š

- **GPU**ï¼šè‡³å°‘3Ã—NVIDIA H100 (80GB)æˆ–åŒç­‰æ€§èƒ½GPU
- **RAM**ï¼šè‡³å°‘128GB
- **å­˜å‚¨**ï¼šè‡³å°‘1TB SSDï¼ˆè®­ç»ƒæ•°æ®ã€æ¨¡å‹æƒé‡å’Œæ£€æŸ¥ç‚¹éœ€è¦å¤§é‡ç©ºé—´ï¼‰
- **é•œåƒ**ï¼šPyTorch 2.0ä»¥ä¸Š + CUDA 11.8ä»¥ä¸Š

åœ¨RunPodä¸Šåˆ›å»ºPodæ—¶ï¼Œé€‰æ‹©åŒ…å«PyTorchçš„é•œåƒï¼Œå¹¶ç¡®ä¿åˆ†é…è¶³å¤Ÿçš„å­˜å‚¨ç©ºé—´ã€‚å¯¹äºåˆå­¦è€…ï¼Œæ¨èé€‰æ‹©å®˜æ–¹çš„PyTorch+CUDAæ¨¡æ¿ã€‚

ç™»å½•RunPodå®ä¾‹åï¼Œé¦–å…ˆæ›´æ–°ç³»ç»Ÿå¹¶å®‰è£…å¿…è¦å·¥å…·ï¼š

```bash
apt-get update
apt-get install -y git wget curl tmux htop
```

### 1.2 LLaMA-Factoryå®‰è£…

LLaMA-Factoryæ˜¯ä¸€ä¸ªåŠŸèƒ½é½å…¨çš„å¼€æºæ¡†æ¶ï¼Œä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒè®¾è®¡ã€‚å…‹éš†ä»“åº“å¹¶è®¾ç½®ï¼š

```bash
cd /workspace
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
```

### 1.3 ä¾èµ–é¡¹å®‰è£…

åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–ï¼š

```bash
python -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -e .
```

å®‰è£…é¢å¤–ä¾èµ–ï¼š

```bash
# å®‰è£…DeepSpeed
pip install deepspeed==0.16.7

# å®‰è£…è®­ç»ƒç›‘æ§å’Œå¯è§†åŒ–å·¥å…·
pip install requests
```

## 2. æ¨¡å‹ä¸æ•°æ®å‡†å¤‡

### 2.1 æ¨¡å‹ä¸‹è½½

ä»Hugging Faceä¸‹è½½Qwen2.5-Coder-7Bæ¨¡å‹ï¼š

```bash
mkdir -p /workspace/models
cd /workspace/models

# ä½¿ç”¨Hugging Face CLIä¸‹è½½
pip install -U huggingface_hub
huggingface-cli download Qwen/Qwen2.5-Coder-7B --local-dir Qwen2.5-Coder-7B --local-dir-use-symlinks False
```

> **æ³¨æ„**ï¼šå¦‚æœé‡åˆ°ç½‘ç»œé—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨`HTTPS_PROXY`ç¯å¢ƒå˜é‡æˆ–è€ƒè™‘å…ˆä¸‹è½½åˆ°æœ¬åœ°å†ä¸Šä¼ åˆ°RunPodã€‚

éªŒè¯æ¨¡å‹æ–‡ä»¶æ˜¯å¦ä¸‹è½½å®Œæ•´ï¼š

```bash
ls -la /workspace/models/Qwen2.5-Coder-7B
```

ç¡®ä¿åŒ…å«ä»¥ä¸‹å…³é”®æ–‡ä»¶ï¼š
- `config.json`
- `model.safetensors.index.json`
- `tokenizer.json`
- ä»¥åŠå…¶ä»–æ¨¡å‹åˆ†ç‰‡æ–‡ä»¶ï¼ˆå¦‚`model-00001-of-00004.safetensors`ï¼‰

### 2.2 æ•°æ®é›†å‡†å¤‡

åˆ›å»ºæ•°æ®ç›®å½•å¹¶å‡†å¤‡æ‚¨çš„æ•°æ®é›†ï¼š

```bash
mkdir -p /workspace/data/finetune_qwen_clean
```

å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼å°†æ•°æ®ä¸Šä¼ åˆ°RunPodï¼š
- ä½¿ç”¨`scp`å‘½ä»¤ä»æœ¬åœ°ä¸Šä¼ 
- ä»å…¬å…±URLä¸‹è½½
- é€šè¿‡RunPodæ–‡ä»¶ç®¡ç†å™¨ä¸Šä¼ 

### 2.3 æ•°æ®æ ¼å¼è¦æ±‚

LLaMA-Factoryæ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼Œä½†å¯¹äºæŒ‡ä»¤å¾®è°ƒï¼Œæ¨èä½¿ç”¨Alpacaæ ¼å¼ï¼š

åˆ›å»ºä¸€ä¸ª`dataset_info.json`æ–‡ä»¶ï¼ŒæŒ‡å®šæ•°æ®é›†ç»“æ„ï¼š

```json
{
  "custom": {
    "file_name": "train_clean.json",
    "file_sha1": null,
    "columns": {
      "prompt": "instruction",
      "query": "input",
      "response": "output"
    },
    "type": "alpaca"
  }
}
```

ç„¶åå‡†å¤‡è®­ç»ƒæ•°æ®`train_clean.json`ï¼Œæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼š

```json
{"instruction":"è¯·æ ¹æ®ä»¥ä¸‹æç¤ºå®Œæˆä»»åŠ¡","input":"å…·ä½“è¾“å…¥å†…å®¹...","output":"æœŸæœ›çš„è¾“å‡ºå†…å®¹..."}
```

æ•°æ®å­—æ®µè¯´æ˜ï¼š
- `instruction`ï¼šä»»åŠ¡æŒ‡ä»¤ï¼Œå‘Šè¯‰æ¨¡å‹è¦åšä»€ä¹ˆ
- `input`ï¼šå…·ä½“çš„è¾“å…¥å†…å®¹ï¼ˆå¯é€‰ï¼Œå¦‚æœä»»åŠ¡ä¸éœ€è¦é¢å¤–è¾“å…¥å¯ä¸ºç©ºï¼‰
- `output`ï¼šæœŸæœ›æ¨¡å‹ç”Ÿæˆçš„è¾“å‡º

æ•°æ®é›†å¤§å°å»ºè®®ï¼šå¤§äº50ä¸‡æ¡æ ·æœ¬ä»¥è·å¾—è‰¯å¥½çš„å¾®è°ƒæ•ˆæœã€‚

## 3. è®­ç»ƒé…ç½®è¯¦è§£

### 3.1 å…¨å‚æ•°å¾®è°ƒvs LoRA

LLaMA-Factoryæ”¯æŒå¤šç§å¾®è°ƒæ–¹æ³•ï¼š

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¨èåœºæ™¯ |
|------|------|------|----------|
| å…¨å‚æ•°å¾®è°ƒ | æ€§èƒ½æ›´å¥½ï¼Œæ¨¡å‹èƒ½åŠ›æ›´å…¨é¢ | éœ€è¦å¤§é‡GPUèµ„æº | æœ‰è¶³å¤Ÿè®¡ç®—èµ„æºï¼Œè¿½æ±‚æœ€ä½³æ•ˆæœ |
| LoRA | èµ„æºéœ€æ±‚å°ï¼Œè®­ç»ƒé€Ÿåº¦å¿« | æ•ˆæœå¯èƒ½ç•¥å·® | èµ„æºæœ‰é™ï¼Œå¿«é€Ÿå®éªŒ |
| QLoRA | èŠ‚çœæ˜¾å­˜ï¼Œé€‚ç”¨äºæ¶ˆè´¹çº§GPU | è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ | ä¸ªäººè®¾å¤‡ï¼Œèµ„æºä¸¥é‡å—é™ |

æœ¬æ•™ç¨‹é€‰æ‹©**å…¨å‚æ•°å¾®è°ƒ**ï¼Œå› ä¸ºï¼š
1. æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„GPUèµ„æºï¼ˆ3Ã—H100ï¼‰
2. è¿½æ±‚æœ€ä½³å¾®è°ƒæ•ˆæœ
3. Qwen2.5-Coder-7Bè§„æ¨¡é€‚ä¸­ï¼Œå…¨å‚æ•°å¾®è°ƒå¯è¡Œ

### 3.2 è®­ç»ƒè„šæœ¬é…ç½®

åˆ›å»ºè‡ªåŠ¨åŒ–è®­ç»ƒè„šæœ¬`auto_gpu_train.sh`ï¼š

```bash
#!/bin/bash
set -e  # å‡ºé”™æ—¶åœæ­¢è„šæœ¬æ‰§è¡Œ

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
echo "æ¿€æ´»Pythonè™šæ‹Ÿç¯å¢ƒ..."
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/venv/bin/activate"

# æ£€æŸ¥ç¯å¢ƒæ˜¯å¦æ­£ç¡®æ¿€æ´»
if ! command -v accelerate &> /dev/null; then
  echo "é”™è¯¯: è™šæ‹Ÿç¯å¢ƒæœªæ­£ç¡®æ¿€æ´»ï¼Œæ‰¾ä¸åˆ°accelerateå‘½ä»¤"
  echo "å°è¯•å®‰è£…å¿…è¦çš„ä¾èµ–..."
  pip install -r requirements.txt
  if ! command -v accelerate &> /dev/null; then
    echo "ä»ç„¶æ— æ³•æ‰¾åˆ°accelerateå‘½ä»¤ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ"
    exit 1
  fi
fi

# æ£€æŸ¥ç£ç›˜ç©ºé—´
free_space=$(df -BG /workspace | awk 'NR==2 {print $4}' | sed 's/G//')
if [ "$free_space" -lt 100 ]; then
  echo "è­¦å‘Š: å¯ç”¨ç©ºé—´å°äº100GBï¼Œå½“å‰å¯ç”¨: ${free_space}GB"
  echo "æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹..."
  python3 disk_monitor.py --clean-only
fi

# è®¾ç½®GPUå’Œè®­ç»ƒå‚æ•°
TOTAL_GPUS=$(nvidia-smi -L | wc -l)
BATCH_SIZE=1
GRAD_ACC_STEPS=16

# ä½¿ç”¨checkpoint-500ä½œä¸ºæ¢å¤ç‚¹
CHECKPOINT_PATH="/workspace/outputs/qwen2.5-full/checkpoint-500"
RESUME_OPTION=""

if [ -d "$CHECKPOINT_PATH" ]; then
  # æ£€æŸ¥checkpoint-500ç›®å½•æ˜¯å¦å®Œæ•´
  if [ -f "$CHECKPOINT_PATH/trainer_state.json" ]; then
    echo "å‘ç°æ£€æŸ¥ç‚¹500ï¼Œå°†ä» $CHECKPOINT_PATH æ¢å¤è®­ç»ƒ"
    RESUME_OPTION="--resume_from_checkpoint $CHECKPOINT_PATH"
  else
    echo "æ£€æŸ¥ç‚¹ç›®å½•ä¸å®Œæ•´ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ"
    # å¯é€‰ï¼šå¤‡ä»½ä¸å®Œæ•´çš„æ£€æŸ¥ç‚¹
    mv "$CHECKPOINT_PATH" "${CHECKPOINT_PATH}_incomplete_$(date +%Y%m%d%H%M%S)"
    mkdir -p /workspace/outputs/qwen2.5-full
  fi
else
  echo "æœªå‘ç°æ£€æŸ¥ç‚¹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ"
  mkdir -p /workspace/outputs/qwen2.5-full
fi

# å¯åŠ¨ç£ç›˜ç›‘æ§ï¼ˆåœ¨åå°è¿è¡Œï¼‰
nohup python3 disk_monitor.py > disk_monitor.log 2>&1 &
MONITOR_PID=$!
echo "å·²å¯åŠ¨ç£ç›˜ç›‘æ§ï¼Œè¿›ç¨‹ID: $MONITOR_PID"

# å¯åŠ¨è®­ç»ƒ
echo "å¼€å§‹è®­ç»ƒ..."
accelerate launch \
  --num_processes=$TOTAL_GPUS \
  --mixed_precision=bf16 \
  --main_process_port=29501 \
  src/train.py \
  --stage sft \
  --model_name_or_path /workspace/models/Qwen2.5-Coder-7B \
  --dataset custom \
  --dataset_dir /workspace/data/finetune_qwen_clean \
  --template qwen \
  --finetuning_type full \
  --output_dir /workspace/outputs/qwen2.5-full \
  --cutoff_len 1024 \
  --preprocessing_num_workers 4 \
  --preprocessing_batch_size 256 \
  --overwrite_cache \
  --gradient_accumulation_steps $GRAD_ACC_STEPS \
  --per_device_train_batch_size $BATCH_SIZE \
  --learning_rate 1e-5 \
  --num_train_epochs 3 \
  --logging_steps 10 \
  --logging_first_step true \
  --save_steps 500 \
  --save_total_limit 3 \
  --bf16 \
  --gradient_checkpointing true \
  --deepspeed ds_config.json \
  --do_train \
  $RESUME_OPTION

# è®­ç»ƒç»“æŸåï¼Œåœæ­¢ç£ç›˜ç›‘æ§
if [ -n "$MONITOR_PID" ]; then
  echo "è®­ç»ƒå®Œæˆï¼Œåœæ­¢ç£ç›˜ç›‘æ§è¿›ç¨‹"
  kill $MONITOR_PID
fi

echo "è®­ç»ƒå·²å®Œæˆï¼"
# é€€å‡ºè™šæ‹Ÿç¯å¢ƒ
deactivate
```

### 3.3 DeepSpeedé…ç½®

åˆ›å»ºDeepSpeedé…ç½®æ–‡ä»¶`ds_config.json`ï¼š

```json
{
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "contiguous_gradients": true,
    "overlap_comm": true,
    "reduce_scatter": true,
    "allgather_partitions": true
  },
  "gradient_accumulation_steps": 16,
  "gradient_clipping": 1.0,
  "train_batch_size": 48,
  "train_micro_batch_size_per_gpu": 1,
  "steps_per_print": "inf",
  "bf16": {
    "enabled": true
  },
  "fp16": {
    "enabled": false
  },
  "zero_allow_untested_optimizer": true
}
```

ä¸»è¦é…ç½®è¯´æ˜ï¼š
- `zero_optimization.stage`ï¼šä½¿ç”¨ZeRO-2ä¼˜åŒ–ï¼Œåœ¨å¤šGPUè®­ç»ƒæ—¶èŠ‚çœæ˜¾å­˜
- `offload_optimizer`ï¼šå°†ä¼˜åŒ–å™¨çŠ¶æ€å¸è½½åˆ°CPUï¼Œè¿›ä¸€æ­¥èŠ‚çœGPUæ˜¾å­˜
- `gradient_accumulation_steps`ï¼šæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œç”¨äºå¢åŠ æœ‰æ•ˆæ‰¹é‡å¤§å°
- `train_batch_size`ï¼šæ€»æ‰¹é‡å¤§å°ï¼ˆæ‰€æœ‰GPUï¼‰
- `train_micro_batch_size_per_gpu`ï¼šæ¯ä¸ªGPUå¤„ç†çš„æ ·æœ¬æ•°
- `bf16`ï¼šå¯ç”¨bfloat16æ··åˆç²¾åº¦è®­ç»ƒï¼ŒH100 GPUä¸Šæ€§èƒ½æœ€ä½³

### 3.4 å¤šGPUè®­ç»ƒè®¾ç½®

å¤šGPUè®­ç»ƒé…ç½®å»ºè®®ï¼š

1. **è®¾ç½®NCCLå‚æ•°**ï¼šå¯¹äºå¤šGPUé€šä¿¡ï¼Œæ·»åŠ ä»¥ä¸‹ç¯å¢ƒå˜é‡å¯æé«˜æ€§èƒ½ï¼š

```bash
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=0
export NCCL_IB_GID_INDEX=3
export NCCL_NET_GDR_LEVEL=2
```

2. **GPUæ‹“æ‰‘æ„ŸçŸ¥**ï¼šæ£€æŸ¥GPUæ‹“æ‰‘å¹¶æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼š

```bash
nvidia-smi topo -m
```

3. **å®éªŒå’Œç›‘æ§**ï¼šåœ¨è®­ç»ƒåˆå§‹é˜¶æ®µå¯†åˆ‡ç›‘æ§GPUåˆ©ç”¨ç‡å’Œå†…å­˜ä½¿ç”¨ï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´å‚æ•°ï¼š

```bash
watch -n 1 nvidia-smi
```

## 4. è®­ç»ƒè¿‡ç¨‹ç®¡ç†

### 4.1 ç£ç›˜ç©ºé—´ç®¡ç†

åˆ›å»ºç£ç›˜ç›‘æ§è„šæœ¬`disk_monitor.py`ï¼š

```python
#!/usr/bin/env python3
import os
import time
import shutil
import argparse
import subprocess
from datetime import datetime

def get_disk_usage():
    """è·å–/workspaceç›®å½•çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ"""
    result = subprocess.run(['df', '-h', '/workspace'], capture_output=True, text=True)
    return result.stdout.strip()

def get_folder_size(path):
    """è·å–æ–‡ä»¶å¤¹å¤§å°"""
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            if os.path.exists(fp):
                total_size += os.path.getsize(fp)
    return total_size

def get_free_space_gb():
    """è·å–å¯ç”¨ç©ºé—´ï¼ˆGBï¼‰"""
    result = subprocess.run(['df', '-BG', '/workspace'], capture_output=True, text=True)
    lines = result.stdout.strip().split('\n')
    if len(lines) >= 2:
        parts = lines[1].split()
        if len(parts) >= 4:
            free_space = parts[3].replace('G', '')
            try:
                return int(free_space)
            except ValueError:
                return 0
    return 0

def clean_old_checkpoints(output_dir, keep_checkpoints=3):
    """æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹ï¼Œä¿ç•™æœ€æ–°çš„Nä¸ª"""
    if not os.path.exists(output_dir):
        print(f"è¾“å‡ºç›®å½• {output_dir} ä¸å­˜åœ¨")
        return

    # æŸ¥æ‰¾æ‰€æœ‰æ£€æŸ¥ç‚¹ç›®å½•
    checkpoint_dirs = []
    for item in os.listdir(output_dir):
        if item.startswith('checkpoint-') and os.path.isdir(os.path.join(output_dir, item)):
            try:
                checkpoint_num = int(item.split('-')[1])
                checkpoint_dirs.append((checkpoint_num, item))
            except ValueError:
                continue

    # æŒ‰æ£€æŸ¥ç‚¹ç¼–å·æ’åº
    checkpoint_dirs.sort(reverse=True)

    # ä¿ç•™æœ€æ–°çš„keep_checkpointsä¸ªï¼Œåˆ é™¤å…¶ä½™çš„
    if len(checkpoint_dirs) > keep_checkpoints:
        for _, dir_name in checkpoint_dirs[keep_checkpoints:]:
            dir_path = os.path.join(output_dir, dir_name)
            print(f"åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {dir_path}")
            try:
                shutil.rmtree(dir_path)
                print(f"æˆåŠŸåˆ é™¤æ£€æŸ¥ç‚¹ç›®å½•: {dir_path}")
            except Exception as e:
                print(f"åˆ é™¤å¤±è´¥: {e}")

def monitor_disk_space(threshold_gb=100, output_dir='/workspace/outputs/qwen2.5-full'):
    """ç›‘æ§ç£ç›˜ç©ºé—´ï¼Œå½“ä½äºé˜ˆå€¼æ—¶æ¸…ç†æ—§æ£€æŸ¥ç‚¹"""
    while True:
        free_space = get_free_space_gb()
        now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        print(f"[{now}] å½“å‰å¯ç”¨ç£ç›˜ç©ºé—´: {free_space}GB")

        if free_space < threshold_gb:
            print(f"è­¦å‘Š: å¯ç”¨ç©ºé—´ä½äºé˜ˆå€¼({threshold_gb}GB)ï¼Œæ¸…ç†æ—§æ£€æŸ¥ç‚¹...")
            clean_old_checkpoints(output_dir)

            # æ¸…ç†åå†æ¬¡æ£€æŸ¥
            new_free_space = get_free_space_gb()
            print(f"æ¸…ç†åå¯ç”¨ç©ºé—´: {new_free_space}GB")

            if new_free_space < threshold_gb:
                print("è­¦å‘Š: æ¸…ç†åç©ºé—´ä»ç„¶ä¸è¶³!")

        # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
        time.sleep(3600)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç£ç›˜ç©ºé—´ç›‘æ§å·¥å…·")
    parser.add_argument("--clean-only", action="store_true", help="åªæ¸…ç†æ—§æ£€æŸ¥ç‚¹ï¼Œä¸è¿›è¡Œç›‘æ§")
    parser.add_argument("--threshold", type=int, default=100, help="ç£ç›˜ç©ºé—´é˜ˆå€¼ï¼ˆGBï¼‰")
    parser.add_argument("--output-dir", type=str, default="/workspace/outputs/qwen2.5-full", help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()

    if args.clean_only:
        print("æ¸…ç†æ—§æ£€æŸ¥ç‚¹...")
        clean_old_checkpoints(args.output_dir)
        print(get_disk_usage())
    else:
        print(f"å¯åŠ¨ç£ç›˜ç©ºé—´ç›‘æ§ï¼Œé˜ˆå€¼: {args.threshold}GB")
        print(get_disk_usage())
        monitor_disk_space(args.threshold, args.output_dir)
```

åœ¨è®­ç»ƒå‰è®¾ç½®è„šæœ¬æƒé™ï¼š

```bash
chmod +x disk_monitor.py
```

### 4.2 æ£€æŸ¥ç‚¹ç®¡ç†

LLaMA-Factoryé»˜è®¤ä¼šåˆ›å»ºæ£€æŸ¥ç‚¹ï¼Œä½†ç®¡ç†ç­–ç•¥éœ€è¦ç‰¹åˆ«æ³¨æ„ï¼š

1. **æ£€æŸ¥ç‚¹é¢‘ç‡**ï¼š
   å»ºè®®æ¯500-1000æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹ï¼Œå¹³è¡¡æ¢å¤èƒ½åŠ›å’Œç£ç›˜ä½¿ç”¨ã€‚

2. **æ£€æŸ¥ç‚¹é™åˆ¶**ï¼š
   é€šè¿‡`--save_total_limit 3`é™åˆ¶ä¿å­˜çš„æ£€æŸ¥ç‚¹æ•°é‡ï¼Œé˜²æ­¢å ç”¨è¿‡å¤šç©ºé—´ã€‚

3. **æ£€æŸ¥ç‚¹æ–­ç‚¹ç»­è®­**ï¼š
   è‹¥è®­ç»ƒä¸­æ–­ï¼Œå¯é€šè¿‡`--resume_from_checkpoint`å‚æ•°æŒ‡å®šæ¢å¤ç‚¹ï¼š

```bash
CHECKPOINT_PATH="/workspace/outputs/qwen2.5-full/checkpoint-500"
# ç¡®ä¿æ£€æŸ¥ç‚¹å®Œæ•´
if [ -f "$CHECKPOINT_PATH/trainer_state.json" ]; then
  RESUME_OPTION="--resume_from_checkpoint $CHECKPOINT_PATH"
fi
```

### 4.3 è®­ç»ƒè¿‡ç¨‹ç›‘æ§

åˆ›å»ºè®­ç»ƒç›‘æ§æ—¥å¿—æŸ¥çœ‹å·¥å…·ï¼š

```bash
# æŸ¥çœ‹æœ€æ–°æŸå¤±å’Œå­¦ä¹ ç‡
tail -f /workspace/outputs/qwen2.5-full/trainer_log.jsonl

# æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ
watch -n 2 nvidia-smi
```

### 4.4 ä½¿ç”¨tmuxè¿›è¡Œåå°è®­ç»ƒ

ä½¿ç”¨tmuxç¡®ä¿è®­ç»ƒåœ¨SSHæ–­å¼€åç»§ç»­è¿è¡Œï¼š

```bash
# å®‰è£…tmuxï¼ˆå¦‚éœ€ï¼‰
apt-get update && apt-get install -y tmux

# åˆ›å»ºæ–°ä¼šè¯
tmux new-session -d -s training "cd /workspace/LLaMA-Factory && ./auto_gpu_train.sh"

# åˆ—å‡ºä¼šè¯
tmux list-sessions

# è¿æ¥åˆ°ä¼šè¯
tmux attach-session -t training

# åˆ†ç¦»ä¼šè¯ï¼ˆåœ¨tmuxå†…ä½¿ç”¨Ctrl+BåæŒ‰Dï¼‰
```

## 5. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 5.1 æ˜¾å­˜ä¸è¶³

å¦‚æœé‡åˆ°æ˜¾å­˜ä¸è¶³ï¼ˆCUDA Out of Memoryï¼‰:

1. **å‡å°æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦**ï¼š
   ```bash
   --cutoff_len 512  # ä»1024å‡å°åˆ°512
   ```

2. **å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼š
   ```bash
   --gradient_checkpointing true
   ```

3. **å¯ç”¨ä¼˜åŒ–å™¨å¸è½½**ï¼š
   åœ¨DeepSpeedé…ç½®ä¸­å¯ç”¨CPUå¸è½½ï¼š
   ```json
   "offload_optimizer": {
     "device": "cpu",
     "pin_memory": true
   }
   ```

4. **å‡å°æ‰¹é‡å¤§å°**ï¼š
   é™ä½`per_device_train_batch_size`å’Œå¢åŠ `gradient_accumulation_steps`

### 5.2 è®­ç»ƒä¸­æ–­æ¢å¤

å¦‚æœè®­ç»ƒä¸­æ–­ï¼š

1. **æ£€æŸ¥æ—¥å¿—æ–‡ä»¶**ï¼š
   ```bash
   tail -n 100 /workspace/LLaMA-Factory/logs/*.log
   ```

2. **éªŒè¯æ£€æŸ¥ç‚¹å®Œæ•´æ€§**ï¼š
   ```bash
   ls -la /workspace/outputs/qwen2.5-full/checkpoint-XXX/
   ```
   ç¡®ä¿åŒ…å«`trainer_state.json`æ–‡ä»¶

3. **ä¿®æ”¹æ¢å¤ç‚¹è·¯å¾„**ï¼š
   åœ¨`auto_gpu_train.sh`ä¸­æ›´æ–°`CHECKPOINT_PATH`å˜é‡

4. **æ¸…é™¤é”æ–‡ä»¶**ï¼ˆå¿…è¦æ—¶ï¼‰ï¼š
   ```bash
   find /workspace/outputs -name "*.lock" -delete
   ```

### 5.3 æ•°æ®å¤„ç†é—®é¢˜

è§£å†³å¸¸è§æ•°æ®é—®é¢˜ï¼š

1. **æ•°æ®æ ¼å¼é”™è¯¯**ï¼š
   ç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼Œæ— é¢å¤–é€—å·ï¼Œæ‰€æœ‰é”®åæœ‰å¼•å·

2. **å­—æ®µåä¸åŒ¹é…**ï¼š
   æ£€æŸ¥`dataset_info.json`ä¸­çš„å­—æ®µæ˜ å°„æ˜¯å¦ä¸å®é™…æ•°æ®åŒ¹é…

3. **OOMé¢„å¤„ç†**ï¼š
   å¦‚æœæ•°æ®é¢„å¤„ç†OOMï¼Œå‡å°`preprocessing_batch_size`

4. **éªŒè¯æ•°æ®é›†**ï¼š
   ```bash
   python -c "import json; data=json.load(open('/workspace/data/finetune_qwen_clean/train_clean.json')); print(f'æ ·æœ¬æ•°: {len(data)}')"
   ```

### 5.4 æ¨¡å‹ä¿å­˜ä¸åŠ è½½

æ¨¡å‹ä¿å­˜ä¸è½¬æ¢ï¼š

1. **æ£€æŸ¥ç”Ÿæˆçš„æ¨¡å‹**ï¼š
   ```bash
   ls -la /workspace/outputs/qwen2.5-full/
   ```

2. **å°†æ¨¡å‹è½¬ä¸ºHFæ ¼å¼**ï¼ˆéDeepSpeedæ ¼å¼ï¼‰ï¼š
   ```bash
   python -c "from transformers import AutoModelForCausalLM; model = AutoModelForCausalLM.from_pretrained('/workspace/outputs/qwen2.5-full/'); model.save_pretrained('/workspace/outputs/qwen2.5-full-hf')"
   ```

3. **æµ‹è¯•æ¨¡å‹ç”Ÿæˆ**ï¼š
   ```bash
   python src/cli_demo.py \
     --model_name_or_path /workspace/outputs/qwen2.5-full \
     --template qwen
   ```

## 6. è¿›é˜¶åŠŸèƒ½

### 6.1 Telegramè®­ç»ƒç›‘æ§

åˆ›å»ºTelegramç›‘æ§è„šæœ¬`training_monitor.py`ï¼š

```python
#!/usr/bin/env python3
import os
import time
import json
import datetime
import subprocess
import requests
import threading

# Telegramé…ç½®
TELEGRAM_BOT_TOKEN = "YOUR_BOT_TOKEN"
TELEGRAM_CHAT_ID = "YOUR_CHAT_ID"
MONITOR_INTERVAL = 20 * 60  # 20åˆ†é’Ÿçš„é—´éš”ï¼Œä»¥ç§’ä¸ºå•ä½

def send_telegram_message(message):
    """å‘é€æ¶ˆæ¯åˆ°Telegram"""
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    data = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": message,
        "parse_mode": "Markdown"
    }

    try:
        response = requests.post(url, data=data)
        if response.status_code != 200:
            print(f"å‘é€Telegramæ¶ˆæ¯å¤±è´¥: {response.text}")
        return response.json()
    except Exception as e:
        print(f"å‘é€Telegramæ¶ˆæ¯å‡ºé”™: {str(e)}")
        return None

def get_gpu_status():
    """è·å–GPUçŠ¶æ€ä¿¡æ¯"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=index,utilization.gpu,memory.used,memory.total,temperature.gpu',
                                '--format=csv,noheader,nounits'],
                                capture_output=True, text=True, check=True)

        gpu_stats = []
        for line in result.stdout.strip().split('\n'):
            if line:
                parts = line.split(', ')
                if len(parts) >= 5:
                    gpu_idx, gpu_util, mem_used, mem_total, temp = parts
                    gpu_stats.append({
                        "index": gpu_idx,
                        "utilization": gpu_util + "%",
                        "memory": f"{mem_used}/{mem_total} MiB",
                        "temperature": temp + "Â°C"
                    })

        return gpu_stats
    except Exception as e:
        return f"è·å–GPUçŠ¶æ€å¤±è´¥: {str(e)}"

def get_training_processes():
    """è·å–è®­ç»ƒè¿›ç¨‹çŠ¶æ€"""
    try:
        result = subprocess.run(['ps', 'aux'], capture_output=True, text=True, check=True)
        processes = result.stdout.strip().split('\n')

        training_processes = []
        for process in processes:
            if 'python' in process and ('accelerate' in process or 'train.py' in process) and 'grep' not in process:
                training_processes.append(process)

        return training_processes
    except Exception as e:
        return f"è·å–è®­ç»ƒè¿›ç¨‹å¤±è´¥: {str(e)}"

def get_tmux_training_status():
    """è·å–tmuxä¼šè¯ä¸­çš„è®­ç»ƒæ—¥å¿—çŠ¶æ€"""
    try:
        result = subprocess.run(['tmux', 'capture-pane', '-p', '-t', 'training'],
                             capture_output=True, text=True, check=True)

        log_lines = result.stdout.strip().split('\n')
        recent_logs = log_lines[-20:]  # è·å–æœ€å20è¡Œæ—¥å¿—

        interesting_logs = []
        for line in recent_logs:
            if any(keyword in line for keyword in ['loss', 'step', 'epoch', 'error', 'Exception', 'ValueError', 'GPU']):
                interesting_logs.append(line)

        return interesting_logs if interesting_logs else ["æœªæ‰¾åˆ°ç›¸å…³è®­ç»ƒæ—¥å¿—"]
    except Exception as e:
        return [f"è·å–tmuxè®­ç»ƒçŠ¶æ€å¤±è´¥: {str(e)}"]

def check_disk_space():
    """æ£€æŸ¥ç£ç›˜ç©ºé—´"""
    try:
        result = subprocess.run(['df', '-h', '/workspace'], capture_output=True, text=True, check=True)
        return result.stdout.strip()
    except Exception as e:
        return f"æ£€æŸ¥ç£ç›˜ç©ºé—´å¤±è´¥: {str(e)}"

def check_training_status():
    """æ£€æŸ¥è®­ç»ƒçŠ¶æ€å¹¶å‘é€é€šçŸ¥"""
    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # æ„å»ºçŠ¶æ€æŠ¥å‘Š
    report = [f"ğŸ¤– *è®­ç»ƒçŠ¶æ€æŠ¥å‘Š* - {current_time}"]

    # è·å–GPUçŠ¶æ€
    gpu_stats = get_gpu_status()
    if isinstance(gpu_stats, list):
        report.append("\nğŸ“Š *GPUçŠ¶æ€*:")
        for gpu in gpu_stats:
            report.append(f"GPU {gpu['index']}: åˆ©ç”¨ç‡ {gpu['utilization']}, å†…å­˜ {gpu['memory']}, æ¸©åº¦ {gpu['temperature']}")
    else:
        report.append(f"\nâš ï¸ GPUçŠ¶æ€: {gpu_stats}")

    # è·å–è®­ç»ƒè¿›ç¨‹
    training_processes = get_training_processes()
    if isinstance(training_processes, list):
        num_processes = len(training_processes)
        report.append(f"\nğŸ”„ *è®­ç»ƒè¿›ç¨‹*: å‘ç° {num_processes} ä¸ªè®­ç»ƒç›¸å…³è¿›ç¨‹")
    else:
        report.append(f"\nâš ï¸ è®­ç»ƒè¿›ç¨‹: {training_processes}")

    # è·å–tmuxè®­ç»ƒçŠ¶æ€
    tmux_logs = get_tmux_training_status()
    report.append("\nğŸ“ *æœ€è¿‘è®­ç»ƒæ—¥å¿—*:")
    for log in tmux_logs[-5:]:  # ä»…æ˜¾ç¤ºæœ€å5è¡Œç›¸å…³æ—¥å¿—
        report.append(log)

    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    disk_space = check_disk_space()
    report.append(f"\nğŸ’¾ *ç£ç›˜ç©ºé—´*:\n{disk_space}")

    # æ£€æŸ¥å¼‚å¸¸æƒ…å†µ
    has_error = False
    if isinstance(gpu_stats, list):
        for gpu in gpu_stats:
            if int(gpu["utilization"].replace("%", "")) < 10:  # GPUåˆ©ç”¨ç‡ä½äº10%
                has_error = True
                report.append(f"\nâš ï¸ *è­¦å‘Š*: GPU {gpu['index']} åˆ©ç”¨ç‡è¿‡ä½ ({gpu['utilization']})")

    if isinstance(training_processes, list) and len(training_processes) == 0:
        has_error = True
        report.append("\nğŸš¨ *è­¦å‘Š*: æœªæ£€æµ‹åˆ°è®­ç»ƒè¿›ç¨‹ï¼")

    for log in tmux_logs:
        if any(err in log for err in ["error", "Error", "ERROR", "Exception", "å¤±è´¥", "CUDA out of memory"]):
            has_error = True
            report.append(f"\nğŸš¨ *é”™è¯¯*: è®­ç»ƒæ—¥å¿—ä¸­å‘ç°é”™è¯¯: {log}")

    # å‘é€æ¶ˆæ¯
    message = "\n".join(report)
    if has_error:
        message = "âš ï¸ *è®­ç»ƒå¼‚å¸¸è­¦æŠ¥* âš ï¸\n" + message

    send_telegram_message(message)

    # è¾“å‡ºåˆ°æ§åˆ¶å°
    print(f"çŠ¶æ€æŠ¥å‘Šå·²å‘é€åˆ°Telegram ({current_time})")
    if has_error:
        print("æ£€æµ‹åˆ°è®­ç»ƒå¼‚å¸¸!")

    return has_error

def monitor_loop():
    """ç›‘æ§å¾ªç¯"""
    print(f"å¯åŠ¨è®­ç»ƒç›‘æ§ï¼Œé—´éš”: {MONITOR_INTERVAL // 60} åˆ†é’Ÿ")

    # å‘é€åˆå§‹åŒ–æ¶ˆæ¯
    send_telegram_message("ğŸ”” *è®­ç»ƒç›‘æ§å·²å¯åŠ¨*\n\nå°†æ¯20åˆ†é’Ÿå‘é€ä¸€æ¬¡çŠ¶æ€æ›´æ–°ï¼Œå¼‚å¸¸æƒ…å†µä¼šç«‹å³é€šçŸ¥ã€‚")

    # ç«‹å³æ‰§è¡Œç¬¬ä¸€æ¬¡æ£€æŸ¥
    has_error = check_training_status()

    while True:
        try:
            # ç­‰å¾…æŒ‡å®šçš„æ—¶é—´
            time.sleep(MONITOR_INTERVAL)
            has_error = check_training_status()

            # å¦‚æœæ£€æµ‹åˆ°é”™è¯¯ï¼Œåˆ™æé«˜ç›‘æ§é¢‘ç‡
            if has_error:
                time.sleep(120)  # 2åˆ†é’Ÿåå†æ¬¡æ£€æŸ¥
                check_training_status()

        except KeyboardInterrupt:
            print("ç›‘æ§ç»ˆæ­¢")
            send_telegram_message("ğŸ›‘ *è®­ç»ƒç›‘æ§å·²åœæ­¢*")
            break
        except Exception as e:
            error_msg = f"ç›‘æ§è„šæœ¬å‡ºé”™: {str(e)}"
            print(error_msg)
            send_telegram_message(f"âš ï¸ *ç›‘æ§è„šæœ¬é”™è¯¯*\n\n{error_msg}")
            time.sleep(300)  # å‡ºé”™åç­‰å¾…5åˆ†é’Ÿå†ç»§ç»­

if __name__ == "__main__":
    # å¯åŠ¨ç›‘æ§çº¿ç¨‹
    monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
    monitor_thread.start()

    try:
        # ä¿æŒä¸»çº¿ç¨‹è¿è¡Œ
        while True:
            time.sleep(60)
    except KeyboardInterrupt:
        print("ç›‘æ§ç¨‹åºé€€å‡º")
    finally:
        send_telegram_message("ğŸ›‘ *è®­ç»ƒç›‘æ§å·²åœæ­¢*")
```

åœ¨tmuxä¼šè¯ä¸­è¿è¡Œç›‘æ§è„šæœ¬ï¼š

```bash
tmux new-session -d -s monitor "/workspace/training_monitor.py"
```

### 6.2 è‡ªåŠ¨æ¸…ç†æ£€æŸ¥ç‚¹

è‡ªåŠ¨æ¸…ç†æ—§æ£€æŸ¥ç‚¹çš„ç­–ç•¥ï¼š

1. åœ¨`auto_gpu_train.sh`ä¸­å·²é›†æˆç£ç›˜ç©ºé—´ç›‘æ§å’Œæ¸…ç†
2. è®­ç»ƒå‚æ•°`--save_total_limit 3`é™åˆ¶ä¿å­˜æ£€æŸ¥ç‚¹æ•°é‡
3. è®¾ç½®`disk_monitor.py`å®šæœŸæ£€æŸ¥ç£ç›˜ç©ºé—´ï¼Œé˜ˆå€¼ä½äº100GBæ—¶æ¸…ç†æ—§æ£€æŸ¥ç‚¹

### 6.3 è®­ç»ƒåè¯„ä¼°

è®­ç»ƒå®Œæˆåè¯„ä¼°æ¨¡å‹ï¼š

```bash
# åœ¨å•ä¸ªç¤ºä¾‹ä¸Šæµ‹è¯•
python src/cli_demo.py \
  --model_name_or_path /workspace/outputs/qwen2.5-full \
  --template qwen

# è¿›è¡ŒåŸºå‡†è¯„ä¼°
python src/evaluate.py \
  --model_name_or_path /workspace/outputs/qwen2.5-full \
  --template qwen \
  --task mmlu \
  --split test \
  --n_shot 5 \
  --batch_size 4
```

## 7. é™„å½•

### 7.1 å®Œæ•´è®­ç»ƒè„šæœ¬

å®Œæ•´çš„`auto_gpu_train.sh`è„šæœ¬å·²åœ¨ä¸Šæ–‡æä¾›ã€‚

### 7.2 ç›‘æ§è„šæœ¬

å®Œæ•´çš„`training_monitor.py`å’Œ`disk_monitor.py`è„šæœ¬å·²åœ¨ä¸Šæ–‡æä¾›ã€‚

### 7.3 èµ„æºé“¾æ¥

- [LLaMA-Factoryå®˜æ–¹æ–‡æ¡£](https://github.com/hiyouga/LLaMA-Factory)
- [Qwen2.5-Coder-7Bæ¨¡å‹é¡µé¢](https://huggingface.co/Qwen/Qwen2.5-Coder-7B)
- [DeepSpeedæ–‡æ¡£](https://www.deepspeed.ai/docs/config-json/)
- [RunPodæ–‡æ¡£](https://docs.runpod.io/)

---

æœ¬æŒ‡å—å¸Œæœ›èƒ½å¸®åŠ©æ‚¨æˆåŠŸå®ŒæˆQwen2.5-Coder-7Bçš„å¾®è°ƒã€‚å¦‚æœ‰é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„å¸®åŠ©ï¼Œè¯·æŸ¥é˜…ç›¸å…³æ–‡æ¡£æˆ–å¯»æ±‚ç¤¾åŒºæ”¯æŒã€‚ç¥æ‚¨è®­ç»ƒé¡ºåˆ©ï¼
